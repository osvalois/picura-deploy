name: Kubernetes Resource Management

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 0 * * 0'  # Run every Sunday at midnight
  workflow_dispatch:  # Allows manual execution

env:
  KUBECONFIG: ./kubeconfig.yaml
  GRAFANA_ADMIN_USER: ${{ secrets.GRAFANA_ADMIN_USER }}
  GRAFANA_ADMIN_PASSWORD: ${{ secrets.GRAFANA_ADMIN_PASSWORD }}

jobs:
  manage-kubernetes-resources:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Set up kubectl
      uses: azure/setup-kubectl@v1
      with:
        version: 'v1.24.0'  # Specify the kubectl version you need

    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > $KUBECONFIG
        chmod 600 $KUBECONFIG

    - name: Debug - Print kubeconfig
      run: |
        echo "Content of kubeconfig (first and last 10 lines):"
        (head -n 10 $KUBECONFIG && echo "..." && tail -n 10 $KUBECONFIG) || echo "Failed to read kubeconfig"

    - name: Debug - Check kubectl config
      run: |
        echo "Current context:"
        kubectl config current-context || echo "Failed to get current context"
        echo "Available contexts:"
        kubectl config get-contexts || echo "Failed to get contexts"
        echo "Clusters:"
        kubectl config get-clusters || echo "Failed to get clusters"

    - name: Debug - Test connection
      run: |
        echo "Attempting to list nodes:"
        kubectl get nodes || echo "Failed to get nodes"
        echo "Attempting to get cluster info:"
        kubectl cluster-info || echo "Failed to get cluster info"

    - name: Set up Helm
      uses: azure/setup-helm@v3
      with:
        version: 'v3.8.0'  # Specify the Helm version you need

    - name: Install or update Monitoring resources
      run: |
        helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
        helm repo add grafana https://grafana.github.io/helm-charts
        helm repo update
        kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
        helm upgrade --install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring || echo "Failed to install Prometheus"
        helm upgrade --install grafana grafana/grafana --namespace monitoring \
          --set adminUser="${GRAFANA_ADMIN_USER}" \
          --set adminPassword="${GRAFANA_ADMIN_PASSWORD}" || echo "Failed to install Grafana"
        
        echo "Verifying Prometheus installation:"
        kubectl get pods -n monitoring -l app=prometheus
        echo "Verifying Grafana installation:"
        kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana

    - name: Install or update Logging resources
      run: |
        helm repo add elastic https://helm.elastic.co
        helm repo update
        kubectl create namespace logging --dry-run=client -o yaml | kubectl apply -f -
        helm upgrade --install elasticsearch elastic/elasticsearch --namespace logging || echo "Failed to install Elasticsearch"
        helm upgrade --install kibana elastic/kibana --namespace logging || echo "Failed to install Kibana"
        
        echo "Verifying Elasticsearch installation:"
        kubectl get pods -n logging -l app=elasticsearch-master
        echo "Verifying Kibana installation:"
        kubectl get pods -n logging -l app=kibana

    - name: Install or update Resource Management
      run: |
        helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
        helm repo update
        helm upgrade --install metrics-server metrics-server/metrics-server --namespace kube-system || echo "Failed to install Metrics Server"
        
        echo "Verifying Metrics Server installation:"
        kubectl get pods -n kube-system -l k8s-app=metrics-server

    - name: Expose services
      run: |
        kubectl patch svc prometheus-kube-prometheus-prometheus -n monitoring -p '{"spec": {"type": "LoadBalancer"}}' || echo "Failed to expose Prometheus"
        kubectl patch svc grafana -n monitoring -p '{"spec": {"type": "LoadBalancer"}}' || echo "Failed to expose Grafana"
        kubectl patch svc elasticsearch-master -n logging -p '{"spec": {"type": "LoadBalancer"}}' || echo "Failed to expose Elasticsearch"
        kubectl patch svc kibana-kibana -n logging -p '{"spec": {"type": "LoadBalancer"}}' || echo "Failed to expose Kibana"

    - name: Debug - Check service status
      run: |
        echo "Prometheus service status:"
        kubectl get svc prometheus-kube-prometheus-prometheus -n monitoring -o wide
        echo "Grafana service status:"
        kubectl get svc grafana -n monitoring -o wide
        echo "Elasticsearch service status:"
        kubectl get svc elasticsearch-master -n logging -o wide
        echo "Kibana service status:"
        kubectl get svc kibana-kibana -n logging -o wide

    - name: Wait for Load Balancers
      run: |
        echo "Waiting for Load Balancers to be assigned public IPs..."
        sleep 180  # Wait for 3 minutes

    - name: Get and display public service URLs
      id: get_urls
      run: |
        echo "Checking service statuses:"
        
        check_service() {
          local namespace=$1
          local service=$2
          echo "Checking $service in namespace $namespace"
          kubectl get svc $service -n $namespace -o wide || echo "Service $service not found in namespace $namespace"
          kubectl describe svc $service -n $namespace || echo "Unable to describe service $service in namespace $namespace"
        }
        
        check_service monitoring prometheus-kube-prometheus-prometheus
        check_service monitoring grafana
        check_service logging elasticsearch-master
        check_service logging kibana-kibana
        
        echo "Attempting to get service IPs:"
        
        get_service_ip() {
          local namespace=$1
          local service=$2
          local ip=$(kubectl get svc $service -n $namespace -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
          if [ -z "$ip" ]; then
            echo "pending"
          else
            echo "$ip"
          fi
        }
        
        PROMETHEUS_IP=$(get_service_ip monitoring prometheus-kube-prometheus-prometheus)
        GRAFANA_IP=$(get_service_ip monitoring grafana)
        ELASTICSEARCH_IP=$(get_service_ip logging elasticsearch-master)
        KIBANA_IP=$(get_service_ip logging kibana-kibana)

        echo "Public URLs (IPs may be pending):"
        echo "Prometheus: http://${PROMETHEUS_IP}:9090"
        echo "Grafana: http://${GRAFANA_IP}:80"
        echo "Elasticsearch: http://${ELASTICSEARCH_IP}:9200"
        echo "Kibana: http://${KIBANA_IP}:5601"

        echo "prometheus_url=http://${PROMETHEUS_IP}:9090" >> $GITHUB_OUTPUT
        echo "grafana_url=http://${GRAFANA_IP}:80" >> $GITHUB_OUTPUT
        echo "elasticsearch_url=http://${ELASTICSEARCH_IP}:9200" >> $GITHUB_OUTPUT
        echo "kibana_url=http://${KIBANA_IP}:5601" >> $GITHUB_OUTPUT

    - name: Debug - Check namespace events
      run: |
        echo "Events in monitoring namespace:"
        kubectl get events -n monitoring
        echo "Events in logging namespace:"
        kubectl get events -n logging

    - name: Create Release Notes
      if: success()
      run: |
        cat << EOF > release_notes.md
        # Service URLs

        ## Monitoring
        - Prometheus: ${{ steps.get_urls.outputs.prometheus_url }}
        - Grafana: ${{ steps.get_urls.outputs.grafana_url }}

        ## Logging
        - Elasticsearch: ${{ steps.get_urls.outputs.elasticsearch_url }}
        - Kibana: ${{ steps.get_urls.outputs.kibana_url }}

        Please note that you may need to use appropriate authentication to access these services.
        If any URLs show as "http://pending", it means the LoadBalancer IP was not yet available. 
        Check the cluster status for the most up-to-date information.
        EOF
        cat release_notes.md

    - name: Create GitHub Release
      if: success()
      uses: actions/create-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: v${{ github.run_number }}
        release_name: Service URLs ${{ github.run_number }}
        body_path: release_notes.md
        draft: false
        prerelease: false

    - name: Show resource usage
      if: success()
      run: |
        kubectl top nodes || echo "Failed to get node resource usage"
        kubectl top pods --all-namespaces || echo "Failed to get pod resource usage"

    - name: Clean up
      if: always()
      run: rm -f $KUBECONFIG